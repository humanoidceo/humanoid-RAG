<!DOCTYPE html>
<html lang="fa" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>humanoid RAG: سیستم Retrieval-Augmented Generation (RAG) آگاه از گفتمان سلسله‌مراتبی با برنامه‌ریزی چند سطحی پرسش</title>
    <link rel="stylesheet" href="style.css">
    <style>
        /* استایل مشابه arXiv با جهت RTL */
        body {
            direction: rtl;
            text-align: right;
            font-family: 'Times New Roman', 'Traditional Arabic', serif;
            max-width: 650px;
            margin: 2em auto;
            padding: 0 1em;
            line-height: 1.6;
            background: #ffffff;
            color: #000000;
            font-size: 11pt;
        }
        
        /* سربرگ مقاله - استایل arXiv */
        .paper-header {
            text-align: center;
            margin-bottom: 2em;
            padding-bottom: 1em;
        }
        .paper-title {
            font-size: 17pt;
            font-weight: bold;
            margin-bottom: 1.5em;
            line-height: 1.3;
        }
        .author-info {
            font-size: 12pt;
            margin: 0.5em 0;
        }
        .affiliation {
            font-size: 10pt;
            color: #000;
            font-style: italic;
            margin: 0.3em 0;
        }
        
        /* چکیده - استایل arXiv */
        .abstract-section {
            margin: 2em 0;
            padding: 0;
            background: transparent;
            border: none;
        }
        .abstract-title {
            font-weight: bold;
            font-size: 12pt;
            text-align: center;
            margin-bottom: 0.5em;
        }
        .abstract-section p {
            text-align: justify;
            text-justify: inter-word;
            margin: 0.8em 0;
        }
        
        /* بخش‌ها - استایل arXiv */
        .section {
            margin: 1.5em 0;
        }
        .section-title {
            font-size: 13pt;
            font-weight: bold;
            margin: 1.2em 0 0.8em 0;
        }
        .subsection-title {
            font-size: 12pt;
            font-weight: bold;
            margin: 1em 0 0.6em 0;
            font-style: italic;
        }
        .subsubsection-title {
            font-size: 11pt;
            font-weight: bold;
            margin: 0.8em 0 0.5em 0;
        }
        
        /* پاراگراف‌ها */
        p {
            text-align: justify;
            text-justify: inter-word;
            margin: 0.8em 0;
            text-indent: 0;
        }
        /* شکل‌ها و تصاویر */
        .figure {
            text-align: center;
            margin: 1em 0;
            padding: 0;
            background: transparent;
            border: none;
        }
        .figure-caption {
            margin-top: 0.5em;
            font-style: italic;
            font-size: 10pt;
        }
        
        /* جداول - استایل arXiv */
        .table {
            width: 100%;
            margin: 1em auto;
            border-collapse: collapse;
        }
        .table th, .table td {
            border: 1px solid #000;
            padding: 0.3em 0.5em;
            text-align: center;
            font-size: 10pt;
        }
        .table th {
            background: transparent;
            font-weight: bold;
            border-bottom: 2px solid #000;
        }
        table {
            margin: 1em auto;
            border-collapse: collapse;
        }
        table th, table td {
            border: 1px solid #000;
            padding: 0.3em 0.5em;
            font-size: 10pt;
        }
        table th {
            font-weight: bold;
            border-bottom: 2px solid #000;
        }
        
        /* معادلات */
        .equation {
            text-align: center;
            margin: 1em 0;
            font-style: italic;
        }
        
        /* الگوریتم‌ها */
        .algorithm {
            margin: 1em 0;
            padding: 0.5em;
            border: 1px solid #000;
            background: transparent;
            font-family: 'Courier New', monospace;
            font-size: 9pt;
            direction: ltr;
            text-align: left;
        }
        
        /* تعاریف و مثال‌ها */
        .definition {
            margin: 1em 0;
            padding: 0.5em 1em;
            border-right: 3px solid #000;
            background: transparent;
        }
        
        /* یادداشت‌ها */
        .note {
            margin: 1em 0;
            padding: 0.5em 1em;
            border: 1px solid #000;
            background: #f9f9f9;
            font-size: 10pt;
        }
        
        /* هشدارها */
        .warning {
            margin: 1em 0;
            padding: 0.5em 1em;
            border: 2px solid #000;
            background: transparent;
            font-weight: bold;
        }
        
        /* منابع */
        .references {
            margin-top: 2em;
        }
        .reference-item {
            margin: 0.5em 0;
            padding-right: 2em;
            text-indent: -2em;
            font-size: 9pt;
            text-align: right;
        }
        
        /* لیست‌ها */
        ul, ol {
            margin: 0.5em 0;
            padding-right: 2em;
        }
        li {
            margin: 0.3em 0;
        }
        
        /* کد */
        code {
            font-family: 'Courier New', monospace;
            font-size: 9pt;
            direction: ltr;
            display: inline;
            background: transparent;
        }
        pre {
            font-family: 'Courier New', monospace;
            font-size: 9pt;
            margin: 1em 0;
            padding: 0.5em;
            overflow-x: auto;
            direction: ltr;
            text-align: left;
            background: transparent;
            border: 1px solid #ccc;
        }
        
        /* استایل‌های اضافی برای شباهت بیشتر به arXiv */
        strong {
            font-weight: bold;
        }
        em {
            font-style: italic;
        }
        h1, h2, h3, h4 {
            font-weight: bold;
            line-height: 1.3;
        }
        
        /* شماره صفحات - استایل arXiv */
        @media print {
            body {
                margin: 0;
                padding: 2em 1em;
            }
            
            @page {
                margin: 2cm;
            }
            
            /* شماره صفحه با JavaScript در هر صفحه چاپ */
            .print-page-number {
                display: block;
                position: fixed;
                bottom: 1cm;
                left: 50%;
                transform: translateX(-50%);
                font-size: 10pt;
                text-align: center;
            }
        }
        
        /* نمایش شماره صفحه در نسخه web فقط - در print مخفی */
        .page-number {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 10pt;
            color: #666;
            background: white;
            padding: 5px 15px;
            border: 1px solid #ddd;
            border-radius: 3px;
            z-index: 1000;
        }
        
        /* Header با arXiv identifier - در print مخفی */
        .arxiv-header {
            position: fixed;
            top: 10px;
            left: 10px;
            font-size: 9pt;
            color: #666;
            background: white;
            padding: 5px;
            border: 1px solid #ddd;
            z-index: 1000;
        }
        
        /* مخفی کردن label‌ها در print */
        @media print {
            .page-number {
                display: none !important;
            }
            .arxiv-header {
                display: none !important;
            }
        }
    </style>
</head>
<body>
    <!-- شماره صفحه -->
    <div class="page-number" id="pageNumber">صفحه ۱</div>
    
    <!-- Header arXiv (اختیاری) -->
    <div class="arxiv-header">
        arXiv:XXXX.XXXXX [cs.CL]
    </div>
    <div class="paper-header">
        <div class="paper-title">
            humanoid RAG: سیستم Retrieval-Augmented Generation (RAG) آگاه از گفتمان سلسله‌مراتبی<br>
            با برنامه‌ریزی چند سطحی پرسش
        </div>
        <div class="author-info">
            فردین ابراهیمی
        </div>
        <div class="affiliation">
            کارشناسی علوم کامپیوتر
        </div>
        <div class="affiliation">
            تحقیق مستقل
        </div>
        <div class="affiliation" style="margin-top: 15px; font-size: 0.9em;">
            تاریخ: فوریه ۲۰۲۶
        </div>
    </div>

    <div class="abstract-section">
        <div class="abstract-title">چکیده</div>
        <p>
            سیستم‌های Retrieval-Augmented Generation (RAG) برای پاسخ به پرسش‌های پیچیده که نیازمند استدلال چند مرحله‌ای هستند، 
            دچار محدودیت‌های اساسی می‌شوند. این مقاله humanoid RAG را معرفی می‌کند: یک چارچوب نوین که 
            برنامه‌ریزی پرسش سلسله‌مراتبی را با مدل‌سازی گفتمان ترکیب می‌کند. سیستم ما شامل دو سطح است: 
            (۱) برنامه‌ریز سطح بالا که پرسش‌های پیچیده را تجزیه کرده، کفایت اطلاعات را تأیید می‌کند و 
            پرسش‌های تکمیلی تولید می‌نماید، و (۲) جستجوگرهای سطح پایین که بازیابی ترکیبی (sparse، dense و web) 
            را انجام می‌دهند. به علاوه، ما مدل‌سازی گفتمان را در دو سطح اِعمال می‌کنیم: درخت‌های ساختار بلاغی 
            (RST) در سطح بخش‌ها برای شناسایی اطلاعات هسته، و گراف بلاغی بین-بخشی برای مدل‌سازی روابط مانند 
            تضاد و بسط. این ساختار گفتمانی، برنامه‌ریزی پاسخ را هدایت کرده و تولید منسجم و دقیق را تضمین می‌کند.
        </p>
        <p>
            <strong>توجه مهم:</strong> این تحقیق تنها با مدل LLaMA 7B و بر روی داده‌های خصوصی آزمایش شده است. 
            هیچ ارزیابی بر روی مجموعه داده‌های معیار عمومی انجام نشده است.
        </p>
        <p>
            <strong>کلمات کلیدی:</strong> Retrieval-Augmented Generation (RAG)، مدل‌سازی گفتمان، برنامه‌ریزی سلسله‌مراتبی، پاسخ به پرسش، استدلال چند مرحله‌ای
        </p>
    </div>

    <div class="section">
        <div class="section-title">۱. مقدمه</div>
        
        <p>
            مدل‌های زبانی بزرگ (LLMs) قابلیت‌های قابل توجهی در تولید متن نشان داده‌اند، اما از محدودیت‌های اساسی 
            رنج می‌برند: دانش پارامتری قدیمی، توهم‌زایی، و عدم دسترسی به اطلاعات خاص حوزه. سیستم‌های 
            Retrieval-Augmented Generation (RAG) با ترکیب بازیابی اسناد خارجی و تولید مبتنی بر LLM، 
            این محدودیت‌ها را کاهش می‌دهند. با این حال، سیستم‌های RAG موجود برای پرسش‌های پیچیدی که نیازمند 
            استدلال چند مرحله‌ای، یکپارچه‌سازی اطلاعات متناقض، یا تصمیم‌گیری درباره کفایت اطلاعات هستند، 
            عملکرد ضعیفی دارند.
        </p>

        <div class="subsection-title">۱.۱ انگیزه</div>
        <p>
            پرسش‌های دنیای واقعی اغلب نیازمند تجزیه به زیرمسائل، جستجوی تکراری، و ترکیب دقیق اطلاعات از 
            برای مثال، پرسشی مانند "آیا Android و iOS هر دو mobile OS هستند؟" 
            نیازمند شناسایی دو زیرپرسش است، بازیابی اطلاعات مربوط به هر OS، 
            و سپس مقایسه آن‌ها. سیستم‌های RAG سنتی که به صورت تک‌مرحله‌ای عمل می‌کنند، در چنین وظایفی 
            ناموفق هستند.
        </p>
        <p>
            به علاوه، اسناد بازیابی شده ممکن است حاوی تضادها باشند (مثلاً اطلاعات قدیمی در مقابل جدید)، 
            اطلاعات ناقص ارائه دهند، یا شامل جملات کم‌اهمیت باشند که LLM را گمراه می‌کنند. فهم روابط گفتمانی 
            بین و درون اسناد برای تولید پاسخ‌های دقیق و منسجم ضروری است.
        </p>

        <div class="subsection-title">۱.۲ مشارکت‌های اصلی</div>
        <p>ما humanoid RAG را ارائه می‌دهیم، یک چارچوب نوین که:</p>
        <ol>
            <li><strong>برنامه‌ریزی سلسله‌مراتبی:</strong> معماری دو سطحی با برنامه‌ریز سطح بالا برای تجزیه پرسش، 
            تأیید کفایت، و تکمیل اطلاعات، و جستجوگرهای سطح پایین برای بازیابی ترکیبی (sparse، dense، web).</li>
            <li><strong>مدل‌سازی گفتمان دوسطحی:</strong> درخت‌های RST درون-بخشی برای شناسایی اطلاعات هسته، 
            و گراف بلاغی بین-بخشی برای مدل‌سازی روابط مانند تضاد و بسط.</li>
            <li><strong>برنامه‌ریزی مبتنی بر گفتمان:</strong> تولید طرح پاسخ بر اساس ساختار گفتمانی برای 
            اطمینان از انسجام و دقت.</li>
        </ol>

        <div class="subsection-title">۱.۳ ساختار مقاله</div>
        <p>
            بقیه این مقاله به شرح زیر سازماندهی شده است: بخش ۲ کارهای مرتبط را بررسی می‌کند. 
            بخش ۳ روش‌شناسی را توضیح می‌دهد. بخش ۴ جزئیات معماری را ارائه می‌دهد. 
            بخش ۵ تحلیل نظری را ارائه می‌کند. بخش ۶ محدودیت‌ها را بحث می‌کند. 
            بخش ۷ نتیجه‌گیری می‌کند.
        </p>
    </div>

    <div class="section">
        <div class="section-title">۲. کارهای مرتبط</div>

        <div class="subsection-title">۲.۱ سیستم‌های Retrieval-Augmented Generation (RAG)</div>
        <p>
            سیستم‌های RAG با ترکیب بازیابی اسناد و تولید متن، دقت و قابلیت استناد LLMها را بهبود می‌بخشند. 
            کارهای اولیه مانند RAG اصلی و کارهای بعدی مانند Self-RAG و FLARE بهبودهایی در بازیابی و 
            تولید ارائه داده‌اند. با این حال، این رویکردها عمدتاً بر پرسش‌های تک‌مرحله‌ای تمرکز دارند 
            و در استدلال چند مرحله‌ای ضعیف هستند.
        </p>

        <div class="subsection-title">۲.۲ استدلال چند مرحله‌ای</div>
        <p>
            روش‌هایی مانند IRCoT و MindSearch با تجزیه پرسش‌های پیچیده به زیرپرسش‌ها و بازیابی تکراری، 
            RAG را برای استدلال چند مرحله‌ای گسترش داده‌اند. کار ما با معرفی برنامه‌ریزی سلسله‌مراتبی و 
            مدل‌سازی گفتمان، از این رویکردها فراتر می‌رود.
        </p>

        <div class="subsection-title">۲.۳ مدل‌سازی گفتمان</div>
        <p>
            نظریه ساختار بلاغی (RST) چارچوبی برای تحلیل ساختار متن ارائه می‌دهد. کارهای اخیر از RST برای 
            بهبود خلاصه‌سازی و تولید متن استفاده کرده‌اند. ما RST را در RAG اِعمال می‌کنیم تا اطلاعات 
            هسته را شناسایی کرده و ترکیب اسناد را بهبود بخشیم.
        </p>

        <div class="subsection-title">۲.۴ کارهای بنیادی</div>
        <p>
            این تحقیق بر پایه دو مقاله اصلی از arXiv است:
        </p>
        <ol>
            <li><strong>LevelRAG:</strong> رویکردی برای Retrieval-Augmented Generation (RAG) سلسله‌مراتبی</li>
            <li><strong>DiscoRAG:</strong> رویکردی برای Retrieval-Augmented Generation (RAG) آگاه از گفتمان</li>
        </ol>
        <p>
            humanoid RAG این دو رویکرد را ترکیب و گسترش می‌دهد تا یک چارچوب یکپارچه ایجاد کند.
        </p>
    </div>

    <div class="section">
        <div class="section-title">۳. روش‌شناسی</div>

        <div class="subsection-title">۳.۱ تعریف مسئله</div>
        <p>
            یک پرسش <em>q</em> و مجموعه‌ای از اسناد <em>D</em> را در نظر بگیرید. هدف تولید پاسخ 
            <em>a</em> است که به طور دقیق به <em>q</em> پاسخ داده و بر اساس اطلاعات بازیابی شده از 
            <em>D</em> باشد. برای پرسش‌های چند مرحله‌ای، <em>q</em> باید به زیرپرسش‌های 
            {<em>q</em>₁, <em>q</em>₂, ..., <em>qₙ</em>} تجزیه شود که هر کدام نیازمند بازیابی و پردازش 
            جداگانه هستند.
        </p>

        <div class="subsection-title">۳.۲ نمای کلی معماری</div>
        <p>
            humanoid RAG از معماری دو سطحی تشکیل شده است:
        </p>
        <ul>
            <li><strong>سطح بالا (برنامه‌ریز):</strong> مسئول تجزیه پرسش، خلاصه‌سازی، تأیید کفایت، و تکمیل اطلاعات</li>
            <li><strong>سطح پایین (جستجوگرها):</strong> سه جستجوگر موازی: sparse (مبتنی بر کلیدواژه)، 
            dense (مبتنی بر معنا)، و web (مبتنی بر اینترنت)</li>
        </ul>

        <div class="figure">
            <pre style="text-align: center; font-family: monospace;">
┌──────────────────────────────────────────────┐
│              پرسش کاربر                      │
└────────────────┬─────────────────────────────┘
                 ▼
┌────────────────────────────────────────────────┐
│         برنامه‌ریز سطح بالا                   │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐    │
│  │  تجزیه   │→│ خلاصه‌سازی│→│  تأیید   │    │
│  └──────────┘  └──────────┘  └──────────┘    │
│                      ↓                         │
│               ┌──────────┐                     │
│               │  تکمیل   │                     │
│               └──────────┘                     │
└────┬────────────┬────────────┬────────────────┘
     │            │            │
┌────▼────┐  ┌───▼────┐  ┌────▼────┐
│جستجوگر  │  │جستجوگر │  │جستجوگر │
│ sparse   │  │ dense │  │   web    │
└────┬────┘  └───┬────┘  └────┬────┘
     │            │            │
     └────────────┼────────────┘
                  ▼
         ┌──────────────────┐
         │  مدل‌سازی گفتمان │
         └──────────────────┘
                  ▼
         ┌──────────────────┐
         │   تولید پاسخ     │
         └──────────────────┘
            </pre>
            <div class="figure-caption">شکل ۱: نمای کلی معماری humanoid RAG</div>
        </div>

        <div class="subsection-title">۳.۳ جریان کاری</div>
        <p>جریان کاری humanoid RAG شامل مراحل زیر است:</p>
        <ol>
            <li><strong>تجزیه (Decompose):</strong> تجزیه <em>q</em> به {<em>q</em>₁, ..., <em>qₙ</em>}</li>
            <li><strong>بازیابی (Retrieve):</strong> برای هر <em>qᵢ</em>، بازیابی موازی از سه منبع</li>
            <li><strong>مدل‌سازی گفتمان (Discourse Modeling):</strong> ساخت درخت‌های RST و گراف بلاغی</li>
            <li><strong>خلاصه‌سازی (Summarize):</strong> استخراج اطلاعات هسته از هر بخش</li>
            <li><strong>تأیید (Verify):</strong> بررسی کفایت اطلاعات برای پاسخ به <em>q</em></li>
            <li><strong>تکمیل (Supplement):</strong> اگر اطلاعات ناکافی است، تولید پرسش‌های اضافی</li>
            <li><strong>برنامه‌ریزی (Plan):</strong> ایجاد طرح پاسخ بر اساس گراف بلاغی</li>
            <li><strong>تولید (Generate):</strong> تولید پاسخ نهایی <em>a</em></li>
        </ol>
    </div>

    <div class="section">
        <div class="section-title">۴. جزئیات معماری</div>

        <div class="subsection-title">۴.۱ برنامه‌ریز سطح بالا</div>
        
        <div class="subsubsection-title">۴.۱.۱ عملیات تجزیه</div>
        <p>
            برنامه‌ریز با استفاده از LLM، پرسش پیچیده <em>q</em> را به مجموعه‌ای از زیرپرسش‌های اتمی 
            {<em>q</em>₁, <em>q</em>₂, ..., <em>qₙ</em>} تجزیه می‌کند. این عملیات با پرامپت طراحی شده 
            که از LLM می‌خواهد پرسش را به سوالات ساده‌تر تقسیم کند، انجام می‌شود.
        </p>
        
        <div class="definition">
            <strong>مثال تجزیه:</strong>
            <p><strong>پرسش:</strong> "آیا smartphone و laptop هر دو می‌توانند به Wi-Fi وصل شوند؟"</p>
            <p><strong>زیرپرسش‌ها:</strong></p>
            <ul>
                <li>q₁: "آیا smartphone می‌تواند به Wi-Fi وصل شود؟"</li>
                <li>q₂: "آیا laptop می‌تواند به Wi-Fi وصل شود؟"</li>
                <li>q₃: "آیا هر دو دستگاه قابلیت اتصال به Wi-Fi را دارند؟"</li>
            </ul>
        </div>

        <div class="subsubsection-title">۴.۱.۲ عملیات خلاصه‌سازی</div>
        <p>
            برای هر زیرپرسش <em>qᵢ</em> و بخش‌های بازیابی شده مربوطه، برنامه‌ریز خلاصه‌ای 
            <em>sᵢ</em> تولید می‌کند که اطلاعات کلیدی را استخراج می‌کند. این عملیات از درخت‌های 
            RST برای اولویت‌بندی جملات هسته استفاده می‌کند.
        </p>

        <div class="subsubsection-title">۴.۱.۳ عملیات تأیید</div>
        <p>
            برنامه‌ریز بررسی می‌کند که آیا خلاصه‌های جمع‌آوری شده {<em>s</em>₁, ..., <em>sₙ</em>} 
            برای پاسخ به پرسش اصلی <em>q</em> کافی هستند. اگر اطلاعات ناکافی باشد، سیستم به 
            عملیات تکمیل می‌رود.
        </p>

        <div class="subsubsection-title">۴.۱.۴ عملیات تکمیل</div>
        <p>
            اگر اطلاعات ناکافی باشد، برنامه‌ریز پرسش‌های تکمیلی جدیدی تولید می‌کند و چرخه 
            بازیابی-خلاصه‌سازی-تأیید را تکرار می‌کند. این فرآیند تا زمانی که اطلاعات کافی باشد، 
            ادامه می‌یابد.
        </p>

        <div class="subsection-title">۴.۲ جستجوگرهای سطح پایین</div>

        <div class="subsubsection-title">۴.۲.۱ جستجوگر sparse</div>
        <p>
            جستجوگر sparse از الگوریتم BM25 برای بازیابی مبتنی بر کلیدواژه استفاده می‌کند. 
            این جستجوگر شامل سه عملیات بازخورد است:
        </p>
        <ul>
            <li><strong>گسترش (Extend):</strong> افزودن کلیدواژه‌های مرتبط برای افزایش پوشش</li>
            <li><strong>فیلتر (Filter):</strong> حذف عبارات نامرتبط با اپراتور NOT</li>
            <li><strong>تأکید (Emphasize):</strong> افزایش وزن کلیدواژه‌های کلیدی</li>
        </ul>

        <div class="definition">
            <strong>مثال پرسش sparse:</strong>
            <pre style="direction: ltr; text-align: left;">
Initial Query: "Wi-Fi" connection
↓ (اگر نتیجه ضعیف)
Extended: "Wi-Fi" connection "wireless"
↓ (اگر هنوز ضعیف)
Filtered: "Wi-Fi" connection -Bluetooth -cable
↓ (اگر لازم باشد)
Emphasized: "Wi-Fi"^2.0 connection
            </pre>
        </div>

        <div class="subsubsection-title">۴.۲.۲ جستجوگر dense</div>
        <p>
            جستجوگر dense از مدل‌های embedding برای بازیابی معنایی استفاده می‌کند. 
            برای بهبود بازیابی، ما از تکنیک HyDE (Hypothetical Document Embeddings) استفاده می‌کنیم: 
            LLM یک سند فرضی تولید می‌کند که پاسخ را شامل می‌شود، سپس embedding این سند برای 
            جستجوی اسناد واقعی مشابه استفاده می‌شود.
        </p>

        <div class="subsubsection-title">۴.۲.۳ جستجوگر web</div>
        <p>
            جستجوگر web برای دسترسی به اطلاعات جاری و گسترده، از APIهای جستجوی web 
            (مانند Bing Search) استفاده می‌کند. این جستجوگر به ویژه برای پرسش‌هایی که نیاز به 
            اطلاعات به‌روز یا اطلاعاتی خارج از مجموعه داده محلی دارند، مفید است.
        </p>

        <div class="subsection-title">۴.۳ مدل‌سازی گفتمان</div>

        <div class="subsubsection-title">۴.۳.۱ درخت‌های RST درون-بخشی</div>
        <p>
            برای هر بخش بازیابی شده، ما یک درخت ساختار بلاغی (RST) می‌سازیم که ساختار گفتمانی 
            داخلی را نشان می‌دهد. درخت RST جملات را به هسته (nucleus) و ماهواره (satellite) تقسیم می‌کند:
        </p>
        <ul>
            <li><strong>هسته:</strong> جملات حاوی اطلاعات اصلی و ضروری</li>
            <li><strong>ماهواره:</strong> جملات حاوی اطلاعات کمکی و پشتیبان</li>
        </ul>

        <div class="definition">
            <strong>مثال درخت RST:</strong>
            <p>متن: "smartphone ها با battery کار می‌کنند. [جمله ۱] charger برای پرکردن battery استفاده می‌شود. 
            [جمله ۲] اما بدون برق، charger کار نمی‌کند. [جمله ۳] به همین دلیل power bank در سفر مفید است. 
            [جمله ۴]"</p>
            <ul>
                <li>هسته: جمله ۳ (اطلاعات اصلی)</li>
                <li>ماهواره: جملات ۱، ۲، ۴ (اطلاعات پس‌زمینه و نتیجه)</li>
            </ul>
        </div>

        <div class="subsubsection-title">۴.۳.۲ گراف بلاغی بین-بخشی</div>
        <p>
            برای مدل‌سازی روابط بین بخش‌های مختلف، ما یک گراف بلاغی <em>G</em> = (<em>V</em>, <em>E</em>) 
            می‌سازیم که در آن هر رأس <em>v</em> ∈ <em>V</em> نشان‌دهنده یک بخش است و هر یال 
            <em>e</em> ∈ <em>E</em> نشان‌دهنده یک رابطه بلاغی است. روابط اصلی عبارتند از:
        </p>
        <ul>
            <li><strong>تضاد (CONTRADICTS):</strong> بخش‌ها حاوی اطلاعات متناقض هستند</li>
            <li><strong>بسط (ELABORATES):</strong> یک بخش اطلاعات بخش دیگر را توضیح می‌دهد</li>
            <li><strong>پس‌زمینه (BACKGROUND_FOR):</strong> یک بخش زمینه برای بخش دیگر فراهم می‌کند</li>
            <li><strong>علت (CAUSES):</strong> یک بخش علت بخش دیگر است</li>
        </ul>

        <div class="definition">
            <strong>مثال گراف بلاغی:</strong>
            <p>پرسش: "آیا همه USB cable ها هم برای charging و هم برای data transfer استفاده می‌شوند؟"</p>
            <ul>
                <li>بخش ۱ (ادعای کلی): "همه USB cable ها هم برای charging و هم برای data transfer هستند"</li>
                <li>بخش ۲ (استثنا): "برخی USB cable ها فقط برای charging ساخته شده‌اند"</li>
                <li>بخش ۳ (توضیح تکمیلی): "USB cable های باکیفیت معمولاً هر دو قابلیت charging و data را دارند"</li>
            </ul>
            <p>روابط:</p>
            <ul>
                <li>بخش ۱ → بخش ۲: تضاد (CONTRADICTS)</li>
                <li>بخش ۳ → بخش ۱: بسط (ELABORATES)</li>
                <li>بخش ۲ → بخش ۳: پس‌زمینه (BACKGROUND_FOR)</li>
            </ul>
        </div>

        <div class="subsubsection-title">۴.۳.۳ برنامه‌ریزی مبتنی بر گفتمان</div>
        <p>
            بر اساس گراف بلاغی، سیستم یک طرح برای تولید پاسخ ایجاد می‌کند. این طرح ترتیب ارائه 
            اطلاعات، نحوه رسیدگی به تضادها، و کلمات انتقالی مناسب را مشخص می‌کند.
        </p>

        <div class="algorithm">
            <strong>الگوریتم ۱: برنامه‌ریزی مبتنی بر گفتمان</strong>
            <pre>
Input: rhetorical graph G = (V, E)
Output: response plan P

1: Initialize plan P = []
2: Identify contradictions in G
3: For each contradiction:
4:    Add historical context (BACKGROUND) to P
5:    Add contrast marker ("but", "however") to P
6:    Add current information to P
7: Identify elaborations in G
8: For each elaboration:
9:    Add main claim to P
10:   Add elaborating details to P
11: Return P
            </pre>
        </div>
    </div>

    <div class="section">
        <div class="section-title">۵. تحلیل نظری و مزایای مورد انتظار</div>

        <div class="warning">
            <strong>⚠️ اطلاعیه مهم:</strong>
            <p>
                این بخش شامل نتایج تجربی از مجموعه داده‌های معیار نیست. این تحقیق تنها بر روی داده‌های 
                خصوصی آزمایش شده است. محتوای زیر چارچوب نظری و مزایای مورد انتظار بر اساس طراحی سیستم 
                را توصیف می‌کند، نه نتایج تجربی از ارزیابی علمی.
            </p>
        </div>

        <div class="subsection-title">۵.۱ مزایای نظری چارچوب</div>
        
        <div class="subsubsection-title">۵.۱.۱ استدلال چند مرحله‌ای</div>
        <p>
            معماری سلسله‌مراتبی humanoid RAG برای رسیدگی به پرسش‌های چند مرحله‌ای طراحی شده است:
        </p>
        <ul>
            <li><strong>تجزیه پرسش:</strong> پرسش‌های پیچیده به زیرپرسش‌های اتمی تجزیه می‌شوند</li>
            <li><strong>بازیابی تکراری:</strong> هر زیرپرسش به طور مستقل پردازش می‌شود</li>
            <li><strong>تأیید کفایت:</strong> سیستم بررسی می‌کند که آیا اطلاعات کافی است</li>
            <li><strong>تکمیل:</strong> پرسش‌های اضافی در صورت نیاز تولید می‌شود</li>
        </ul>
        <p>
            <strong>مزیت مورد انتظار:</strong> عملکرد بهتر در پرسش‌هایی که نیاز به چندین مرحله استدلال دارند، 
            مانند پرسش‌های مقایسه‌ای یا استنتاجی.
        </p>

        <div class="subsubsection-title">۵.۱.۲ همکاری بازیابی ترکیبی</div>
        <p>
            استفاده همزمان از سه جستجوگر مزایای متکامل ارائه می‌دهد:
        </p>
        <ul>
            <li><strong>جستجوگر sparse:</strong> دقیق برای اسامی خاص، تاریخ‌ها، و عبارات دقیق</li>
            <li><strong>جستجوگر dense:</strong> خوب برای فهم معنایی و پرسش‌های مفهومی</li>
            <li><strong>جستجوگر web:</strong> دسترسی به اطلاعات جاری و پوشش گسترده</li>
        </ul>
        <p>
            <strong>مزیت مورد انتظار:</strong> پوشش بازیابی بالاتر با ترکیب نقاط قوت مکمل، کاهش موارد 
            "عدم یافت" که در روش‌های تک‌منبعی رخ می‌دهد.
        </p>

        <div class="subsubsection-title">۵.۱.۳ آگاهی از گفتمان</div>
        <p>
            مدل‌سازی گفتمان دو سطحی چندین مزیت نظری ارائه می‌دهد:
        </p>
        <ul>
            <li><strong>درخت‌های RST:</strong> شناسایی اطلاعات هسته، کاهش اثر "گم‌شدن در وسط"</li>
            <li><strong>گراف بلاغی:</strong> تشخیص تضادها، جلوگیری از ترکیب اطلاعات متناقض</li>
            <li><strong>برنامه‌ریزی:</strong> تولید پاسخ‌های منسجم با ساختار منطقی</li>
        </ul>
        <p>
            <strong>مزیت مورد انتظار:</strong> دقت بالاتر در اولویت‌بندی اطلاعات، رسیدگی بهتر به تضادها، 
            و پاسخ‌های منسجم‌تر.
        </p>

        <div class="subsection-title">۵.۲ مطالعه موردی مفهومی</div>
        
        <div class="definition">
            <strong>مثال: "آیا همه TV ها smart هستند؟"</strong>
            
            <p><strong>RAG استاندارد (رفتار مورد انتظار):</strong></p>
            <p>ممکن است تعریف "smart TV" را بازیابی کرده و 
            بدون توجه به تفاوت مدل‌های قدیمی و جدید، "بله" پاسخ دهد.</p>
            
            <p><strong>humanoid RAG (رفتار مورد انتظار):</strong></p>
            <ol>
                <li><strong>بازیابی:</strong> منابع متعدد شامل اطلاعات دسته‌بندی و زمان عرضه
                    <ul>
                        <li>بخش ۱: "smart TV می‌تواند به Internet وصل شود و app اجرا کند"</li>
                        <li>بخش ۲: "TV های قدیمی فقط video input دارند و smart نیستند"</li>
                        <li>بخش ۳: "امروزه بسیاری از TV های جدید smart هستند"</li>
                    </ul>
                </li>
                <li><strong>تحلیل گفتمان:</strong> تمایز بین مدل‌های قدیمی و جدید شناسایی می‌شود</li>
                <li><strong>برنامه‌ریزی:</strong> توضیح تفاوت دسته‌ها، سپس جمع‌بندی</li>
                <li><strong>تولید:</strong> "نه، همه TV ها smart نیستند. TV های قدیمی ساده‌اند، 
                اما بسیاری از TV های جدید smart هستند."</li>
            </ol>
            
            <p><strong>بینش کلیدی:</strong> سیستم بین دسته‌های مختلف محصول تمایز می‌گذارد 
            و پاسخ دقیق‌تری ارائه می‌دهد.</p>
        </div>

        <div class="subsection-title">۵.۳ مشارکت‌های مورد انتظار اجزا</div>
        
        <div class="subsubsection-title">۵.۳.۱ عملیات برنامه‌ریز سطح بالا</div>
        <ul>
            <li><strong>تجزیه:</strong> حیاتی برای شکستن پرسش‌های پیچیده - انتظار می‌رود برای پرسش‌های 
            چند مرحله‌ای تأثیرگذارترین باشد</li>
            <li><strong>خلاصه‌سازی:</strong> اطلاعات را فشرده می‌کند در حالی که حقایق کلیدی را حفظ می‌کند</li>
            <li><strong>تأیید:</strong> اطمینان حاصل می‌کند که اطلاعات بازیابی شده برای پاسخ به پرسش کافی است</li>
            <li><strong>تکمیل:</strong> شکاف‌های اطلاعاتی را با تولید پرسش‌های اضافی پر می‌کند</li>
        </ul>

        <div class="subsubsection-title">۵.۳.۲ اصلاح جستجوگر sparse</div>
        <ul>
            <li><strong>گسترش:</strong> کلیدواژه‌های مرتبط را برای گسترش جستجو اضافه می‌کند</li>
            <li><strong>فیلتر:</strong> نویز را با حذف عبارات نامرتبط کاهش می‌دهد</li>
            <li><strong>تأکید:</strong> اهمیت عبارات کلیدی را افزایش می‌دهد</li>
            <li><strong>مزیت مورد انتظار:</strong> بازیابی دقیق‌تر موجودیت‌ها در مقایسه با BM25 ساده</li>
        </ul>

        <div class="subsubsection-title">۵.۳.۳ اجزای گفتمان</div>
        <ul>
            <li><strong>درخت‌های RST:</strong> به اولویت‌بندی جملات مهم کمک می‌کنند، اثر "گم‌شدن در وسط" را کاهش می‌دهند</li>
            <li><strong>گراف‌های بلاغی:</strong> تشخیص و حل تضاد را امکان‌پذیر می‌کنند</li>
            <li><strong>طرح برنامه‌ریزی:</strong> راهنمایی ساختاریافته برای تولید منسجم فراهم می‌کند</li>
        </ul>

    <div class="section">
        <div class="section-title">۶. محدودیت‌ها و ملاحظات</div>

        <div class="subsection-title">۶.۱ محدودیت‌های ارزیابی</div>
        
        <div class="warning">
            <strong>⚠️ محدودیت‌های اساسی تحقیق:</strong>
            <ul>
                <li><strong>مدل:</strong> تنها با LLaMA 7B آزمایش شده است</li>
                <li><strong>داده:</strong> تنها بر روی داده‌های خصوصی ارزیابی شده است</li>
                <li><strong>معیارها:</strong> بر روی مجموعه داده‌های معیار عمومی (PopQA، NQ، TriviaQA، 
                HotpotQA، 2WikiMultiHopQA) ارزیابی نشده است</li>
                <li><strong>مقایسه:</strong> هیچ مقایسه آماری با سیستم‌های پایه انجام نشده است</li>
                <li><strong>مطالعات حذفی:</strong> هیچ مطالعه حذفی رسمی برای اندازه‌گیری مشارکت اجزا انجام نشده است</li>
            </ul>
        </div>

        <div class="subsection-title">۶.۲ محدودیت‌های فنی</div>
        
        <div class="subsubsection-title">۶.۲.۱ پیچیدگی محاسباتی</div>
        <p>
            humanoid RAG به دلیل چندین فراخوانی LLM (برای تجزیه، خلاصه‌سازی، تأیید، تکمیل، و تولید) 
            و پردازش گفتمان، هزینه محاسباتی بالاتری نسبت به RAG ساده دارد. این امر ممکن است زمان پاسخ 
            و هزینه در محیط‌های تولیدی را افزایش دهد.
        </p>

        <div class="subsubsection-title">۶.۲.۲ دقت تجزیه گفتمان</div>
        <p>
            ساخت درخت‌های RST و گراف‌های بلاغی به دقت تجزیه‌کننده گفتمان بستگی دارد. خطاها در 
            شناسایی روابط بلاغی می‌تواند به تولید پاسخ نادرست منجر شود. دقت تجزیه با حوزه و کیفیت 
            متن متفاوت است.
        </p>

        <div class="subsubsection-title">۶.۲.۳ وابستگی به مهندسی پرامپت</div>
        <p>
            عملکرد سیستم به شدت به کیفیت پرامپت‌های طراحی شده برای هر عملیات بستگی دارد. 
            پرامپت‌های ضعیف می‌توانند به تجزیه نادرست، خلاصه‌های ناکافی، یا تأیید اشتباه منجر شوند.
        </p>

        <div class="subsubsection-title">۶.۲.۴ پوشش بازیابی</div>
        <p>
            علی‌رغم استفاده از سه جستجوگر، هنوز ممکن است برخی اطلاعات مربوط بازیابی نشوند، 
            به ویژه اگر در مجموعه داده محلی وجود نداشته باشند یا به شکلی فرموله شده باشند که 
            با پرسش تطابق نداشته باشد.
        </p>

        <div class="subsection-title">۶.۳ محدودیت‌های کاربردی</div>
        
        <div class="subsubsection-title">۶.۳.۱ تأخیر</div>
        <p>
            ماهیت تکراری سیستم (چرخه تجزیه-بازیابی-خلاصه‌سازی-تأیید-تکمیل) می‌تواند به تأخیر 
            قابل توجه در پاسخ منجر شود، به ویژه برای پرسش‌های پیچیده که نیاز به چندین تکرار دارند.
        </p>

        <div class="subsubsection-title">۶.۳.۲ هزینه</div>
        <p>
            فراخوانی‌های متعدد LLM و عملیات بازیابی می‌توانند هزینه را افزایش دهند، به ویژه اگر از 
            APIهای تجاری استفاده شود. برای برنامه‌های با حجم بالا، این هزینه می‌تواند قابل توجه باشد.
        </p>

        <div class="subsection-title">۶.۴ کارهای آینده</div>
        <p>کارهای آینده می‌تواند شامل موارد زیر باشد:</p>
        <ul>
            <li>ارزیابی بر روی مجموعه داده‌های معیار عمومی</li>
            <li>مطالعات حذفی برای اندازه‌گیری دقیق مشارکت هر جزء</li>
            <li>آزمایش با مدل‌های زبانی مختلف (GPT-4، Claude، Mistral، و غیره)</li>
            <li>بهینه‌سازی برای کاهش تأخیر و هزینه</li>
            <li>بهبود دقت تجزیه گفتمان</li>
            <li>توسعه تکنیک‌های کش‌سازی برای کاهش فراخوانی‌های LLM تکراری</li>
        </ul>
    </div>

    <div class="section">
        <div class="section-title">۷. راهنمای پیاده‌سازی</div>

        <div class="subsection-title">۷.۱ پشته فناوری پیشنهادی</div>
        
        <div class="table">
            <table>
                <thead>
                    <tr>
                        <th>مؤلفه</th>
                        <th>ابزارهای پیشنهادی</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>مدل زبانی (برنامه‌ریز)</strong></td>
                        <td>LLaMA 7B، GPT-3.5-turbo، Mistral-7B، Claude</td>
                    </tr>
                    <tr>
                        <td><strong>جستجوگر sparse</strong></td>
                        <td>Elasticsearch، BM25 (rank_bm25)، Apache Solr</td>
                    </tr>
                    <tr>
                        <td><strong>جستجوگر dense</strong></td>
                        <td>FAISS، Pinecone، Weaviate، ChromaDB، sentence-transformers</td>
                    </tr>
                    <tr>
                        <td><strong>جستجوگر web</strong></td>
                        <td>Bing Search API، SerpAPI، Google Custom Search</td>
                    </tr>
                    <tr>
                        <td><strong>تجزیه گفتمان</strong></td>
                        <td>مدل‌های transformer سفارشی، مدل‌های NLI (RoBERTa-NLI)</td>
                    </tr>
                    <tr>
                        <td><strong>چارچوب</strong></td>
                        <td>LangChain، LlamaIndex، Haystack</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="subsection-title">۷.۲ معماری پیشنهادی</div>
        
        <div class="algorithm">
            <strong>الگوریتم ۲: خط‌لوله humanoid RAG</strong>
            <pre>
class HumanoidRAG:
    def __init__(self, llm, sparse, dense, web):
        self.planner = HighLevelPlanner(llm)
        self.sparse_searcher = SparseSearcher(sparse)
        self.dense_searcher = DenseSearcher(dense)
        self.web_searcher = WebSearcher(web)
        self.discourse_parser = DiscourseParser()
    
    def answer(self, query):
        # مرحله ۱: تجزیه
        sub_queries = self.planner.decompose(query)
        
        # مرحله ۲: بازیابی
        all_chunks = []
        for sq in sub_queries:
            sparse_chunks = self.sparse_searcher.search(sq)
            dense_chunks = self.dense_searcher.search(sq)
            web_chunks = self.web_searcher.search(sq)
            all_chunks.extend([sparse_chunks, dense_chunks, web_chunks])
        
        # مرحله ۳: مدل‌سازی گفتمان
        rst_trees = self.discourse_parser.build_rst_trees(all_chunks)
        rhetorical_graph = self.discourse_parser.build_graph(all_chunks)
        
        # مرحله ۴: خلاصه‌سازی
        summaries = []
        for sq, chunks in zip(sub_queries, grouped_chunks):
            nucleus_info = extract_nucleus(chunks, rst_trees)
            summary = self.planner.summarize(sq, nucleus_info)
            summaries.append(summary)
        
        # مرحله ۵: تأیید و تکمیل
        while not self.planner.verify(query, summaries):
            new_query = self.planner.supplement(query, summaries)
            # تکرار بازیابی و خلاصه‌سازی...
        
        # مرحله ۶: برنامه‌ریزی و تولید
        plan = self.discourse_parser.create_plan(rhetorical_graph)
        answer = self.planner.generate(query, summaries, plan)
        
        return answer
            </pre>
        </div>

        <div class="subsection-title">۷.۳ نکات پیاده‌سازی</div>
        <ul>
            <li><strong>ساده شروع کنید:</strong> ابتدا RAG ساده را پیاده‌سازی کنید، سپس به تدریج 
            برنامه‌ریزی سلسله‌مراتبی و مدل‌سازی گفتمان را اضافه کنید</li>
            <li><strong>کش‌سازی:</strong> نتایج بازیابی و پاسخ‌های LLM را کش کنید تا هزینه و تأخیر کاهش یابد</li>
            <li><strong>پردازش موازی:</strong> سه جستجوگر را به صورت موازی اجرا کنید</li>
            <li><strong>مدیریت خطا:</strong> راه‌حل‌های جایگزین برای زمانی که تجزیه گفتمان یا تأیید شکست می‌خورد</li>
            <li><strong>نظارت:</strong> ثبت عملکرد هر جستجوگر و مؤلفه برای بهینه‌سازی</li>
        </ul>
    </div>

    <div class="section">
        <div class="section-title">۸. نتیجه‌گیری</div>
        
        <p>
            ما humanoid RAG را معرفی کردیم، یک چارچوب نوین که برنامه‌ریزی پرسش سلسله‌مراتبی را با 
            مدل‌سازی گفتمان ترکیب می‌کند تا چالش‌های سیستم‌های RAG سنتی را در پاسخ به پرسش‌های 
            پیچیده برطرف کند. معماری دو سطحی ما - با برنامه‌ریز سطح بالا برای تجزیه پرسش و تأیید 
            کفایت، و جستجوگرهای سطح پایین برای بازیابی ترکیبی - رویکردی ساختاریافته برای استدلال 
            چند مرحله‌ای فراهم می‌کند.
        </p>
        
        <p>
            مدل‌سازی گفتمان دو سطحی ما - درخت‌های RST درون-بخشی برای شناسایی اطلاعات هسته و 
            گراف بلاغی بین-بخشی برای مدل‌سازی روابط مانند تضاد - امکان تولید پاسخ‌های دقیق‌تر و 
            منسجم‌تر را فراهم می‌کند. این ساختار گفتمانی، برنامه‌ریزی پاسخ را هدایت می‌کند و 
            اطمینان حاصل می‌کند که تضادها به درستی رسیدگی شده و اطلاعات به ترتیب منطقی ارائه می‌شوند.
        </p>
        
        <div class="warning">
            <strong>⚠️ یادآوری مهم:</strong>
            <p>
                این تحقیق یک مطالعه مفهومی و نظری است که تنها بر روی داده‌های خصوصی با مدل LLaMA 7B 
                آزمایش شده است. ارزیابی جامع بر روی مجموعه داده‌های معیار عمومی و مطالعات حذفی برای 
                تأیید مزایای ادعا شده لازم است.
            </p>
        </div>
        
        <p>
            کارهای آینده شامل ارزیابی بر روی معیارهای استاندارد، آزمایش با مدل‌های زبانی مختلف، 
            و بهینه‌سازی برای محیط‌های تولیدی است. ما امیدواریم که humanoid RAG بتواند به عنوان 
            پایه‌ای برای تحقیقات بیشتر در زمینه سیستم‌های RAG آگاه از گفتمان عمل کند.
        </p>
    </div>

    <div class="section">
        <div class="section-title">تشکر و قدردانی</div>
        <p>
            این تحقیق بر پایه کارهای پیشگام در زمینه Retrieval-Augmented Generation (RAG) و مدل‌سازی گفتمان بنا شده است. 
            ما از جامعه منبع باز و نویسندگان مقالات LevelRAG و DiscoRAG برای کار الهام‌بخش آن‌ها 
            سپاسگزاریم.
        </p>
    </div>

    <div class="references">
        <div class="section-title">منابع</div>
        
        <div class="note">
            <strong>📚 منابع بنیادی:</strong>
            <p>این تحقیق بر اساس دو مقاله اصلی از arXiv است:</p>
            <ol>
                <li><strong>LevelRAG:</strong> تحقیق درباره Retrieval-Augmented Generation (RAG) سلسله‌مراتبی (موجود در arXiv)</li>
                <li><strong>DiscoRAG:</strong> تحقیق درباره Retrieval-Augmented Generation (RAG) آگاه از گفتمان (موجود در arXiv)</li>
            </ol>
        </div>

        <div class="reference-item">
            [1] T. B. Brown et al., "Language Models are Few-Shot Learners," <em>NeurIPS</em>, 2020.
        </div>
        
        <div class="reference-item">
            [2] H. Touvron et al., "LLaMA: Open and Efficient Foundation Language Models," 
            <em>arXiv:2302.13971</em>, 2023.
        </div>
        
        <div class="reference-item">
            [3] P. Lewis et al., "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks," 
            <em>NeurIPS</em>, 2020.
        </div>
        
        <div class="reference-item">
            [4] A. Asai et al., "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection," 
            <em>ICLR</em>, 2024.
        </div>
        
        <div class="reference-item">
            [5] H. Trivedi et al., "Interleaving Retrieval with Chain-of-Thought Reasoning," 
            <em>arXiv:2212.10509</em>, 2023.
        </div>
        
        <div class="reference-item">
            [6] W. C. Mann and S. A. Thompson, "Rhetorical Structure Theory: Toward a Functional Theory of Text Organization," 
            <em>Text</em>, vol. 8, no. 3, pp. 243-281, 1988.
        </div>
        
        <div class="reference-item">
            [7] Z. Chen et al., "MindSearch: Mimicking Human Minds Elicits Deep AI Searcher," 
            <em>arXiv:2407.20183</em>, 2024.
        </div>
        
        <div class="reference-item">
            [8] L. Gao et al., "Precise Zero-Shot Dense Retrieval without Relevance Labels," 
            <em>arXiv:2212.10496</em>, 2022.
        </div>
        
        <div class="reference-item">
            [9] L. Wang et al., "Query2doc: Query Expansion with Large Language Models," 
            <em>arXiv:2303.07678</em>, 2023.
        </div>
        
        <div class="reference-item">
            [10] Y. Gao et al., "Retrieval-Augmented Generation for Large Language Models: A Survey," 
            <em>arXiv:2312.10997</em>, 2024.
        </div>
    </div>

    <div class="section" style="margin-top: 60px; padding-top: 20px; border-top: 2px solid #333;">
        <div class="section-title">اطلاعات نویسنده</div>
        <div class="warning">
            <strong>نویسنده:</strong> فردین ابراهیمی<br>
            <strong>مدرک:</strong> کارشناسی علوم کامپیوتر<br>
            <strong>وضعیت:</strong> تحقیق مستقل - بدون وابستگی سازمانی<br>
            <strong>حمایت مالی:</strong> این تحقیق توسط هیچ سازمان یا مؤسسه‌ای حمایت مالی نشده است
        </div>
        </div>
    </div>

    <script>
        // تبدیل اعداد انگلیسی به فارسی
        function toPersianNumber(num) {
            var persianDigits = ['۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹'];
            return num.toString().replace(/\d/g, function(digit) {
                return persianDigits[digit];
            });
        }
        
        // محاسبه شماره صفحه بر اساس scroll
        window.addEventListener('scroll', function() {
            var pageHeight = 1122; // ارتفاع یک صفحه A4 (px)
            var scrollPosition = window.scrollY;
            var pageNumber = Math.floor(scrollPosition / pageHeight) + 1;
            
            var pageNumberElement = document.getElementById('pageNumber');
            if (pageNumberElement) {
                // نمایش با فرمت "صفحه X از Y" با اعداد فارسی
                var totalHeight = document.body.scrollHeight;
                var totalPages = Math.ceil(totalHeight / pageHeight);
                pageNumberElement.textContent = 'صفحه ' + toPersianNumber(pageNumber) + ' از ' + toPersianNumber(totalPages);
            }
        });
        
        // تنظیم اولیه شماره صفحه
        window.addEventListener('load', function() {
            var pageHeight = 1122;
            var totalHeight = document.body.scrollHeight;
            var totalPages = Math.ceil(totalHeight / pageHeight);
            
            var pageNumberElement = document.getElementById('pageNumber');
            if (pageNumberElement) {
                pageNumberElement.textContent = 'صفحه ' + toPersianNumber(1) + ' از ' + toPersianNumber(totalPages);
            }
            
            console.log('تعداد کل صفحات: ' + totalPages);
        });
        
        // افزودن شماره صفحات فارسی برای print
        window.addEventListener('beforeprint', function() {
            var pageHeight = 1122; // ارتفاع صفحه A4
            var totalHeight = document.body.scrollHeight;
            var totalPages = Math.ceil(totalHeight / pageHeight);
            
            // حذف شماره‌های قبلی
            var oldNumbers = document.querySelectorAll('.print-page-number');
            oldNumbers.forEach(function(el) { el.remove(); });
            
            // افزودن شماره صفحه به هر بخش
            var sections = document.querySelectorAll('.section, .abstract-section, .paper-header');
            var currentPage = 1;
            
            sections.forEach(function(section, index) {
                var sectionTop = section.offsetTop;
                var pageNum = Math.floor(sectionTop / pageHeight) + 1;
                
                if (pageNum !== currentPage) {
                    currentPage = pageNum;
                    
                    // ایجاد عنصر شماره صفحه
                    var pageNumberDiv = document.createElement('div');
                    pageNumberDiv.className = 'print-page-number';
                    pageNumberDiv.style.position = 'absolute';
                    pageNumberDiv.style.top = (pageNum * pageHeight - 50) + 'px';
                    pageNumberDiv.textContent = toPersianNumber(pageNum);
                    
                    document.body.appendChild(pageNumberDiv);
                }
            });
        });
        
        // پاکسازی بعد از print
        window.addEventListener('afterprint', function() {
            var oldNumbers = document.querySelectorAll('.print-page-number');
            oldNumbers.forEach(function(el) { el.remove(); });
        });
    </script>

</body>
</html>
